---
title: "ADHDCYCLE_daily_analysis"
author: "Tory Eisenlohr-Moul"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    number_sections: true
---

# ---- Setup ----
- Turns of Scientific Notation
- Saves 'current_date' as a string in YYYYMMDD format
- Sets the number of digits (3) to display in output
- Saves Current Date as a string in YYYYMMDD format
- Sets the 'output_folder' as path to the output folder for saving files
```{r setup}

source("scripts/options_packages.R")
source("scripts/set-output-folder.R")

<<<<<<< Updated upstream
<<<<<<< Updated upstream
```
=======
=======
>>>>>>> Stashed changes
# Create a new output folder with the current date
current_date <- format(Sys.Date(), "%Y%m%d")  # Produces date as YYYYMMDD
output_folder <- paste0(
  "~/Library/CloudStorage/Box-Box/00 - CLEAR Lab (Locked Folders)/02 - Data Management, Analysis, and Papers/Studies_Projects/UKALC/03_analytic_projects/CYCADHD_PRIMARY/03_code_dataedits_output/",
  current_date
) # Define the output folder with the current date
# State the output folder
cat("Output folder set as: ", output_folder, "\n")
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes


```{r set-output-folder, quiet=TRUE, message=FALSE, warning=FALSE}
source("scripts/set-output-folder.R")

# Create the output folder if needed
if (!dir.exists(output_folder)) {
  dir.create(output_folder)
}
```



# ---- Store Global Lists of final variable names and labels ----
- Create Lists of Variables for Analysis
-   @dv_list: List of dependent variables for analysis
-   @hormlist: List of hormone variables for analysis
-   @alldailyvars: dv_list and hormlist combined

```{r make lists, cache=TRUE}

dv_list <- c(
  "CSS_Inatt",
  "CSS_HypImp",
  "score_pinball",
  "score_robot",
  "BDEFS_Total",
  "BDEFS_WM_avg",
  "BDEFS_RI_avg",
  "UPPS_NU_avg",
  "UPPS_PU_avg",
  "UPPS_Premed_avg",
  "UPPS_Persev_avg",
  "UPPS_Sens_avg",
  "DEBQ_Total",
  "CSS_Inatt_Count",
  "CSS_Hyp_Count",
  "CSS_Imp_Count",
  "CSS_HypImp_Count",
  "DRSP_1",
  "DRSP_2",
  "DRSP_3",
  "DRSP_4",
  "DRSP_5",
  "DRSP_6",
  "DRSP_7",
  "DRSP_8",
  "DRSP_9",
  "DRSP_10",
  "DRSP_11",
  "DRSP_12",
  "DRSP_13",
  "DRSP_14",
  "DRSP_15",
  "DRSP_16",
  "DRSP_17",
  "DRSP_18",
  "DRSP_19",
  "DRSP_20",
  "DRSP_21",
  "DRSP_22",
  "DRSP_23"
) %>% noquote()



# Rename the variables based on DRSP items
# This creates a named vector to assign meaningful labels to each variable for better interpretation in the plots
names(dv_list) <- c(
  "Inattention Sx", 
  "Hyperactivity/Imp Sx", 
  "Pinball Score",
  "Robot Score",
  "BDEFS Total",
  "BDEFS Working Memory",
  "BDEFS Resp Inhibition",
  "Negative Urgency",
  "Positive Urgency",
  "Lack of Premeditation",
  "Lack of Perseverance",
  "Sensation Seeking",
  "DEBQ Total",
  "Inattention Sx Count",
  "Hyperactivity Sx Count",
  "Impulsivity Sx Count",
  "Hyp/Imp Sx Count",
  "Depressed Mood", # DRSP_1
  "Hopelessness", # DRSP_2
  "Worthlessness/Guilt", # DRSP_3
  "Anxiety/Tension", # DRSP_4
  "Mood Swings", # DRSP_5
  "Rejection Sensitivity", # DRSP_6
  "Anger/Irritability", # DRSP_7
  "Interpersonal Conflict", # DRSP_8
  "Less Interest", # DRSP_9
  "Difficulty Concentrating", # DRSP_10
  "Lethargy/Fatigue", # DRSP_11
  "Incr Appetite/Overate", # DRSP_12
  "Food Cravings", # DRSP_13
  "Hypersomnia", # DRSP_14
  "Insomnia", # DRSP_15
  "Overwhelm/Can't Cope", # DRSP_16
  "Out of Control", # DRSP_17
  "Breast Tenderness", # DRSP_18
  "Swelling/Bloat/Wt Gain", # DRSP_19
  "Joint/Muscle Pain", # DRSP_20
  "Headache", # DRSP_21
  "Work/School Imp", 
  "Relational Imp"
)

# Rename the variables based on DRSP items
names(dv_list) <- c(
  "Depressed Mood", # DRSP_1
  "hopelessness", # DRSP_2
  "Worthlessness/Guilt", # DRSP_3
  "Anxiety/Tension", # DRSP_4
  "Mood Swings", # DRSP_5
  "Rejection Sensitivity", # DRSP_6
  "Anger/Irritability", # DRSP_7
  "Interpersonal Conflict", # DRSP_8
  "Less Interest", # DRSP_9
  "Difficulty Concentrating", # DRSP_10
  "Lethargy/Fatigue", # DRSP_11
  "Increased Appetite/Overate", # DRSP_12
  "Food Cravings", # DRSP_13
  "Hypersomnia", # DRSP_14
  "Insomnia", # DRSP_15
  "Overwhelm/Can't Cope", # DRSP_16
  "Out of Control", # DRSP_17
  "Breast Tenderness", # DRSP_18
  "Swelling/Bloating/Weight Gain", # DRSP_19
  "Joint/Muscle Pain", # DRSP_20
  "Headache", # DRSP_21
  "Work/School Impairment", 
  "Relational Impairment",
  "Inattention Symptoms", 
  "Hyperactivity/Impulsivity Symptoms", 
  "Pinball Score",
  "Robot Score",
  "BDEFS Total",
  "BDEFS Working Memory",
  "BDEFS Response Inhibition",
  "Negative Urgency",
  "Positive Urgency",
  "Lack of Premeditation",
  "Lack of Perseverance",
  "Sensation Seeking",
  "DEBQ Total",
  "Inattention Sx Count",
  "Hyperactivity Sx Count",
  "Impulsivity Sx Count",
  "Hyperactivity/Impulsivity Sx Count"
)

hormlist <- c(
  "E2",
  "P4",
  "LH"
) %>% noquote()

#Create another list that combines both dv_list and hormlist above
alldailyvars <- c(dv_list, hormlist) %>% noquote()
```

# Input dataset - CLEAN AS OF 2024-10-20

```{r}
# Load your CSV file (adjust the path to your actual file location)
df <- read_csv("~/Library/CloudStorage/Box-Box/00 - CLEAR Lab (Locked Folders)/02 - Data Management, Analysis, and Papers/Studies_Projects/CYCLEADHD/03_analytic_projects/CYCADHD_PRIMARY/03_code_dataedits_output/adhd_daily_df_20241020.csv")
source("scripts/print.variable.names.R")
print.variable.names(df)

str(df$id)
 #set id as factor
df$id <- as.factor(df$id)
```

# ---- Setting Variable Formats  ----
- Convert daterated to Date format using lubridate if necessary (ensure date format matches)
```{r updating formats, include=TRUE}

convert_date_if_needed <- function(df, date_column) {
  # Check if the date column is in Date format
  if (!inherits(df[[date_column]], "Date")) {
    df <- df %>%
      mutate({{ date_column }} := lubridate::mdy(!!sym(date_column))) # Convert to mm/dd/yyyy format if not already in Date format
  }
  return(df)
}

# Call
df <- convert_date_if_needed(df, "daterated")

```

# ---- Sort df by id and daterated ----
```{r sort df}
df <- df %>%
  arrange(id, daterated)

source("scripts/count_rows_ids.R")
count_rows_ids(df)
```


# NOTE: Cristina Pinheiro EF Analyses Happen about here; see RMD file. 



# ---- END SETUP AND PREP  ----

# ---- START ANALYSES   ----

# Check Association between cycleday variable and days since starting

```{r, cache=TRUE}

df$TubeNumber <- as.numeric(df$TubeNumber) # Convert TubeNumber to numeric

df_first <- df %>%
  group_by(id) %>%
  filter(TubeNumber == 1) %>%
  ungroup()

hist(df_first$scaled_cycleday, breaks = 10)
hist(df_first$scaled_cycleday_ov, breaks = 10)



#cor(x=as.numeric(df$TubeNumber), y=df$scaled_cycleday, use="complete.obs")

#plot(x=as.numeric(df$TubeNumber), y=df$scaled_cycleday, use="complete.obs")

```

# 2024-10-20 - PLOTS - LUTEAL then FOLLICULAR - (OVULATION to OVULATION)
# Saved in 20241020 folder
```{r plot-ov-ov-means, eval=FALSE, cache=TRUE}

# Define the function to generate plots for a list of DVs
plot_DVs <- function(dv_list, df) {
  
  # Create person-standardized (z-scored within each participant) versions of each DV
  for (DV in dv_list) {
    zd_var <- paste0(DV, ".zd")  # Name for the person-standardized version
    
    # Add the person-standardized columns for each DV
    df <- df %>%
      group_by(id) %>%
      mutate(!!zd_var := scale(!!sym(DV), center = TRUE, scale = TRUE)) %>%
      ungroup()
  }
  
  # Calculate person-rescaled E2 and P4 (scaled within each person, sd = 0.5)
  df <- df %>%
    group_by(id) %>%
    mutate(E2.rescale = scale(E2, center = TRUE, scale = sd(E2, na.rm=TRUE)/0.5), #changed here
           P4.rescale = scale(P4, center = TRUE, scale = sd(P4, na.rm=TRUE)/0.5)) %>% #changed here
    ungroup()
  
  # Indicators for menstrual bleeding
  bleeding_indicator <- data.frame(xmin = .5, xmax = 0.7, ymin = -Inf, ymax = Inf)
  
  for (DV in dv_list) {
    
    if (DV %in% c("E2", "P4", "LH")) {
      # Raw plot for E2, P4, LH
      dv <- df %>%
        group_by(cycleday_10perc) %>%
        summarise(mean_value = mean(!!sym(DV), na.rm = TRUE),
                  se_value = sd(!!sym(DV), na.rm = TRUE) / sqrt(n()))  # SE calculation
      
      p1 <- ggplot(dv, aes(x = cycleday_10perc, y = mean_value)) +
        geom_ribbon(aes(ymin = mean_value - se_value, ymax = mean_value + se_value), 
                    fill = "black", alpha = 0.1) +
        geom_point(size = 1) +
        geom_line(size = 1) +
        ggtitle(paste("Raw", DV, "Plot")) +
        xlab("Cycle Day (10% bins)") + ylab(paste(DV, "Mean")) +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5, size = 14),
              plot.margin = margin(15, 5, 50, 5)) +
        coord_cartesian(clip = "off") +
        geom_rect(data = bleeding_indicator, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
                  fill = "red", alpha = 0.2, inherit.aes = FALSE)
      
      # Plot the data
      print(p1)
      
      ggsave(filename = paste0(output_folder, "/", DV, "_", "raw", ".png"), plot = last_plot(), width = 8, height = 6)

      
    } else {
      # For DVs that are NOT E2, P4, LH
      
      # Raw data points and line for the dependent variable
      dv <- df %>%
        group_by(cycleday_10perc) %>%
        summarise(mean_value = mean(!!sym(DV), na.rm = TRUE),
                  se_value = sd(!!sym(DV), na.rm = TRUE) / sqrt(n()))  # SE calculation
      
    # Person-rescaled E2 and P4 lines (thin dotted)  <- Changed variable names here
      E2_std <- df %>%
        group_by(cycleday_10perc) %>%
        summarise(mean_value = mean(E2.rescale, na.rm = TRUE)) #changed here

      p4_std <- df %>%
        group_by(cycleday_10perc) %>%
        summarise(mean_value = mean(P4.rescale, na.rm = TRUE))  #changed here
      
      p1 <- ggplot(dv, aes(x = cycleday_10perc, y = mean_value)) +
        geom_ribbon(aes(ymin = mean_value - se_value, ymax = mean_value + se_value), 
                    fill = "black", alpha = 0.3) +
        geom_point(size = 1) +
        geom_line(size = 2) +
        # Thin dotted lines for E2 and P4
        # geom_line(data = E2_std, aes(x = cycleday_10perc, y = mean_value), 
        #           linetype = "dotted", color = "blue", size = 1, alpha = 0.6) +  # E2
        # geom_line(data = p4_std, aes(x = cycleday_10perc, y = mean_value), 
        #           linetype = "dotted", color = "forestgreen", size = 1, alpha = 0.6) +  # P4
        ggtitle(paste("Raw", DV, "Plot")) +
        xlab("Cycle Day (10% bins)") + ylab(paste(DV, "Mean")) +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5, size = 14),
              plot.margin = margin(15, 5, 50, 5)) +
        coord_cartesian(clip = "off") +
        geom_rect(data = bleeding_indicator, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
                  fill = "red", alpha = 0.2, inherit.aes = FALSE)
      
      # Plot the data
      print(p1)
      
      # Deviations plot
      dev_var <- paste0(DV, ".d")
      dv <- df %>%
        group_by(cycleday_10perc) %>%
        summarise(mean_value = mean(!!sym(dev_var), na.rm = TRUE),
                  se_value = sd(!!sym(dev_var), na.rm = TRUE) / sqrt(n()))  # SE calculation
      
      p2 <- ggplot(dv, aes(x = cycleday_10perc, y = mean_value)) +
        geom_ribbon(aes(ymin = mean_value - se_value, ymax = mean_value + se_value), 
                    fill = "black", alpha = 0.3) +
        geom_point(size = 1) +
        geom_line(size = 2) +
        # geom_line(data = E2_std, aes(x = cycleday_10perc, y = mean_value), 
        #           linetype = "dotted", color = "blue", size = 1, alpha = 0.6) +  # E2
        # geom_line(data = p4_std, aes(x = cycleday_10perc, y = mean_value), 
        #           linetype = "dotted", color = "forestgreen", size = 1, alpha = 0.6) +  # P4
        ggtitle(paste("Deviations", DV, "Plot")) +
        xlab("Cycle Day (10% bins)") + ylab(paste(DV, "Deviation Mean")) +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5, size = 14),
              plot.margin = margin(15, 5, 50, 5)) +
        coord_cartesian(clip = "off") +
        geom_rect(data = bleeding_indicator, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
                  fill = "red", alpha = 0.2, inherit.aes = FALSE)
      
      print(p2)
      
            ggsave(filename = paste0(output_folder, "/", DV, "_", "dev", ".png"), plot = last_plot(), width = 8, height = 6)

      
      # Person-standardized outcome (zd)
      zd_var <- paste0(DV, ".zd")
      dv <- df %>%
        group_by(cycleday_10perc) %>%
        summarise(mean_value = mean(!!sym(zd_var), na.rm = TRUE),
                  se_value = sd(!!sym(zd_var), na.rm = TRUE) / sqrt(n()))  # SE calculation
      
      p3 <- ggplot(dv, aes(x = cycleday_10perc, y = mean_value)) +
        geom_ribbon(aes(ymin = mean_value - se_value, ymax = mean_value + se_value), 
                    fill = "black", alpha = 0.3) +
        geom_point(size = 1) +
        geom_line(size = 2) +
        geom_line(data = E2_std, aes(x = cycleday_10perc, y = mean_value), 
                  linetype = "solid", color = "forestgreen", size = 1, alpha = 0.9) +  # E2
        geom_line(data = p4_std, aes(x = cycleday_10perc, y = mean_value), 
                  linetype = "solid", color = "red", size = 1, alpha = 0.8) +  # P4
        ggtitle(paste("Person-standardized", DV, "Plot")) +
        xlab("Cycle Day (10% bins)") + ylab(paste(DV, "Person-standardized Mean")) +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5, size = 14),
              plot.margin = margin(15, 5, 50, 5)) +
        coord_cartesian(clip = "off") +
        geom_rect(data = bleeding_indicator, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
                  fill = "red", alpha = 0.2, inherit.aes = FALSE)
      
      print(p3)
                  ggsave(filename = paste0(output_folder, "/", DV, "_", "zd", ".png"), plot = last_plot(), width = 8, height = 6)

      
      # Rolling deviations plot
      roll_dev_var <- paste0(DV, ".d.5roll")
      dv <- df %>%
        group_by(cycleday_10perc) %>%
        summarise(mean_value = mean(!!sym(roll_dev_var), na.rm = TRUE),
                  se_value = sd(!!sym(roll_dev_var), na.rm = TRUE) / sqrt(n()))  # SE calculation
      
      p4 <- ggplot(dv, aes(x = cycleday_10perc, y = mean_value)) +
        geom_ribbon(aes(ymin = mean_value - se_value, ymax = mean_value + se_value), 
                    fill = "black", alpha = 0.3) +
        geom_point(size = 1) +
        geom_line(size = 2) +
        # geom_line(data = E2_std, aes(x = cycleday_10perc, y = mean_value), 
        #           linetype = "dotted", color = "blue", size = 1, alpha = 0.6) +  # E2
        # geom_line(data = p4_std, aes(x = cycleday_10perc, y = mean_value), 
        #           linetype = "dotted", color = "forestgreen", size = 1, alpha = 0.6) +  # P4
        ggtitle(paste("Rolling Deviations", DV, "Plot")) +
        xlab("Cycle Day (10% bins)") + ylab(paste(DV, "Rolling Deviation Mean")) +
        theme_minimal() +
        theme(plot.title = element_text(hjust = 0.5, size = 14),
              plot.margin = margin(15, 5, 50, 5)) +
        coord_cartesian(clip = "off") +
        geom_rect(data = bleeding_indicator, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), 
                  fill = "red", alpha = 0.2, inherit.aes = FALSE)
      
      # Plot the data
      print(p4)
                  ggsave(filename = paste0(output_folder, "/", DV, "_", "rolld", ".png"), plot = last_plot(), width = 8, height = 6)

    }
  }
}

plot_DVs(dv_list, df)

```

# 2024-10-20 - PLOTS - FOLLICULAR then LUTEAL - (MENSES to MENSES)
# Saved in 20241020 folder
```{r plot-mens-mens-means, eval=FALSE, cache=TRUE}


# Define the function to generate plots for a list of DVs
# Args:
#   dv_list: A named vector of dependent variables (DVs) to be plotted.
#   df: A data frame containing the data for analysis and plotting.
#   output_folder: The folder path where the generated plots should be saved.
plot_DVs_M2M <- function(dv_list, df, output_folder) {
  
  # Create person-standardized (z-scored within each participant) versions of each DV
  for (DV in dv_list) {
    zd_var <- paste0(DV, ".zd")  # Name for the person-standardized version
    
    # Add the person-standardized columns for each DV
    df <- df %>%
      group_by(id) %>%
      mutate(!!zd_var := scale(!!sym(DV), center = TRUE, scale = TRUE)) %>%
      ungroup()
  }
  
  # Calculate person-rescaled E2 and P4 (scaled within each person, sd = 0.5)
  # This transformation helps in visual comparison by keeping the SD consistent across participants
  df <- df %>%
    group_by(id) %>%
    mutate(E2.rescale = scale(E2, center = TRUE, scale = (sd(E2, na.rm = TRUE)*1.75)),
           P4.rescale = scale(P4, center = TRUE, scale = (sd(P4, na.rm = TRUE)*1.75))) %>%
    ungroup()
  
  # Indicators for menstrual bleeding (static values for highlighting cycle day range)
  bleeding_indicator <- data.frame(xmin = 0, xmax = 0.2, ymin = -Inf, ymax = Inf)
  
  # Loop through each DV to generate plots
  for (DV in dv_list) {
    # Get the label for the DV from dv_list
    DV_label <- names(dv_list)[which(dv_list == DV)]
    
    # Calculate mean and standard error for the raw dependent variable
    dv <- df %>%
      group_by(cycledayov_10perc) %>%
      summarise(mean_value = mean(!!sym(DV), na.rm = TRUE),
                se_value = sd(!!sym(DV), na.rm = TRUE) / sqrt(n()))  # SE calculation
    
    # Calculate mean and standard error for the person-z-scored outcome (zd)
    zd_var <- paste0(DV, ".zd")
    dv_zd <- df %>%
      group_by(cycledayov_10perc) %>%
      summarise(mean_value = mean(!!sym(zd_var), na.rm = TRUE),
                se_value = sd(!!sym(zd_var), na.rm = TRUE) / sqrt(n()))  # SE calculation
    
    # Generate plot for the person-standardized outcome (zd)
p3 <- ggplot(dv_zd, aes(x = cycledayov_10perc, y = mean_value)) +
  # Plotting the mean values for the person-standardized outcome with LOESS smoothing
  geom_smooth(method = "loess", span = .7, size = 1.5, se = FALSE, color = "black") +  # Use geom_smooth for the main DV with CI
  # Plotting the rescaled E2 and P4 values with LOESS smoothing
  geom_smooth(data = df, aes(x = cycledayov_10perc, y = E2.rescale, color = "E2"), method = "loess", span = 0.5, 
              linetype = "dashed", size = 1, alpha = 0.9, se = FALSE) +
  geom_smooth(data = df, aes(x = cycledayov_10perc, y = P4.rescale, color = "P4"), method = "loess", span = 0.5, 
              linetype = "dashed", size = 1, alpha = 0.8, se = FALSE) +
  # Manual color settings for different lines in the plot
  scale_color_manual(values = c("E2" = "forestgreen", "P4" = "red")) +
  # Set axis labels and plot title
  xlab("Cycle Day (10% bins)") + ylab(paste("Smoothed Person-Z-Scored", DV_label)) +
  # Apply minimal theme for cleaner look
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14),
        plot.margin = margin(15, 5, 50, 5),
        legend.title = element_blank(),
        legend.position = "right") +
  coord_cartesian(clip = "off") +
  ggtitle(label=DV_label)+
  # Highlight menstrual bleeding days with a red rectangle
  geom_rect(data = bleeding_indicator, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), 
            fill = "red", alpha = 0.2, inherit.aes = FALSE)

# Print the plot to the graphics device
print(p3)
    
    # Save the plot to the specified output folder
    ggsave(filename = file.path(output_folder, paste0(DV, "_", "ov_zd_noSE", ".png")), plot = p3, width = 8, height = 6)
  }
}

# Call the function with the dataframe and output folder
plot_DVs_M2M(dv_list, df, output_folder)

```


# 2024-02-14 - FITTING GAMMs for E2, P4, LH - Have to do this first because otherwise the function below breaks
# Saved in 20250214 Folder with same dataset as in October but with factor set to ID. 

```{r}

#############################
### 1. E2 Analysis Section ###
#############################

# Plot and transform the E2 data
hist(df$E2, main = "Histogram of E2", xlab = "E2")
df$E2log <- log(df$E2 + 1)   # Log-transform E2
hist(df$E2log, main = "Histogram of log(E2+1)", xlab = "log(E2+1)")

# Ensure 'id' is a factor
df$id <- as.factor(df$id)

# Fit the GAMM model
gamm_E2 <- gam(E2log ~ s(id, bs = "re") + 
                 s(id, scaled_cycleday, bs = "re") + 
                 s(scaled_cycleday), 
               data = df, method = "REML")

# Save model summary
summary_E2 <- capture.output(summary(gamm_E2))
writeLines(summary_E2, file.path(output_folder, "E2_gamm_summary.txt"))

# Hierarchical partitioning
var_part_E2 <- gam.hp::gam.hp(gamm_E2)
var_part_E2_output <- capture.output(var_part_E2$hierarchical.partitioning)
writeLines(var_part_E2_output, file.path(output_folder, "E2_var_partitioning.txt"))

# Create a dataset for predictions
E2dat <- expand.grid(scaled_cycleday = seq(-1, 1, by = 0.1), id = 0)
pred_E2 <- predictions(gamm_E2, newdata = E2dat, type = "response",
                       transform = function(x) exp(x) - 1)
E2dat$estimate <- pred_E2$estimate
E2dat$conf.low <- pred_E2$conf.low
E2dat$conf.high <- pred_E2$conf.high

# Create the E2 plot
E2plot <- ggplot(E2dat, aes(x = scaled_cycleday, y = estimate)) +
  scale_x_continuous(
    limits = c(-1, 1),
    breaks = seq(-1, 1, by = 0.50),
    labels = c("0%L", "50%L", "Menses Onset", "50%F", "Ovulation")
  ) +
  labs(x = "", y = "Salivary Estradiol") +
  geom_rect(aes(xmin = -0.04, xmax = 0.04, ymin = -Inf, ymax = Inf),
            fill = "grey70", alpha = 0.2, color = "white") +
  geom_rect(aes(xmin = 0.92, xmax = 1, ymin = -Inf, ymax = Inf),
            fill = "grey87", alpha = 0.2, color = "white") +
  geom_line(size = 1, show.legend = TRUE) +
  theme_minimal(base_size = 16)

print(E2plot)
ggsave(
  filename = file.path(output_folder, "E2_gamm_plot.png"),
  plot = E2plot,
  width = 10,
  height = 8,
  dpi = 300,
  device = ragg::agg_png,
  bg = "white"
)

#############################
### 2. P4 Analysis Section ###
#############################

hist(df$P4, main = "Histogram of P4", xlab = "P4")
df$P4log <- log(df$P4 + 1)
hist(df$P4log, main = "Histogram of log(P4+1)", xlab = "log(P4+1)")

gamm_P4 <- gam(P4log ~ s(id, bs = "re") + 
                 s(scaled_cycleday, id, bs = "re") + 
                 s(scaled_cycleday, k = 20),
               data = df, method = "REML")

summary_P4 <- capture.output(summary(gamm_P4))
writeLines(summary_P4, file.path(output_folder, "P4_gamm_summary.txt"))

var_part_P4 <- gam.hp::gam.hp(gamm_P4)
var_part_P4_output <- capture.output(var_part_P4$hierarchical.partitioning)
writeLines(var_part_P4_output, file.path(output_folder, "P4_var_partitioning.txt"))

P4dat <- expand.grid(scaled_cycleday = seq(-1, 1, by = 0.1), id = 0)
pred_P4 <- predictions(gamm_P4, newdata = P4dat, type = "response",
                       transform = function(x) exp(x) - 1)
P4dat$estimate <- pred_P4$estimate
P4dat$conf.low <- pred_P4$conf.low
P4dat$conf.high <- pred_P4$conf.high

P4plot <- ggplot(P4dat, aes(x = scaled_cycleday, y = estimate)) +
  scale_x_continuous(
    limits = c(-1, 1),
    breaks = seq(-1, 1, by = 0.50),
    labels = c("0%L", "50%L", "Menses Onset", "50%F", "Ovulation")
  ) +
  labs(x = "", y = "Salivary Progesterone") +
  geom_rect(aes(xmin = -0.04, xmax = 0.04, ymin = -Inf, ymax = Inf),
            fill = "grey70", alpha = 0.2, color = "white") +
  geom_rect(aes(xmin = 0.92, xmax = 1, ymin = -Inf, ymax = Inf),
            fill = "grey87", alpha = 0.2, color = "white") +
  geom_line(size = 1, show.legend = TRUE) +
  theme_minimal(base_size = 16)

print(P4plot)
ggsave(
  filename = file.path(output_folder, "P4_gamm_plot.png"),
  plot = P4plot,
  width = 10,
  height = 8,
  dpi = 300,
  device = ragg::agg_png,
  bg = "white"
)

#############################
### 3. LH Analysis Section ###
#############################

hist(df$LH, main = "Histogram of LH", xlab = "LH")
df$LHlog <- log(df$LH + 1)
hist(df$LHlog, main = "Histogram of log(LH+1)", xlab = "log(LH+1)")

gamm_LH <- gam(LHlog ~ s(id, bs = "re") + 
                 s(scaled_cycleday, id, bs = "re") + 
                 s(scaled_cycleday, k = 20),
               data = df, method = "REML")

summary_LH <- capture.output(summary(gamm_LH))
writeLines(summary_LH, file.path(output_folder, "LH_gamm_summary.txt"))

var_part_LH <- gam.hp::gam.hp(gamm_LH)
var_part_LH_output <- capture.output(var_part_LH$hierarchical.partitioning)
writeLines(var_part_LH_output, file.path(output_folder, "LH_var_partitioning.txt"))

LHdat <- expand.grid(scaled_cycleday = seq(-1, 1, by = 0.1), id = 0)
pred_LH <- predictions(gamm_LH, newdata = LHdat, type = "response",
                       transform = function(x) exp(x) - 1)
LHdat$estimate <- pred_LH$estimate
LHdat$conf.low <- pred_LH$conf.low
LHdat$conf.high <- pred_LH$conf.high

LHplot <- ggplot(LHdat, aes(x = scaled_cycleday, y = estimate)) +
  scale_x_continuous(
    limits = c(-1, 1),
    breaks = seq(-1, 1, by = 0.50),
    labels = c("0%L", "50%L", "Menses Onset", "50%F", "Ovulation")
  ) +
  labs(x = "", y = "Salivary Progesterone") +
  geom_rect(aes(xmin = -0.04, xmax = 0.04, ymin = -Inf, ymax = Inf),
            fill = "grey70", alpha = 0.2, color = "white") +
  geom_rect(aes(xmin = 0.92, xmax = 1, ymin = -Inf, ymax = Inf),
            fill = "grey87", alpha = 0.2, color = "white") +
  geom_line(size = 1, show.legend = TRUE) +
  theme_minimal(base_size = 16)

print(LHplot)
ggsave(
  filename = file.path(output_folder, "LH_gamm_plot.png"),
  plot = LHplot,
  width = 10,
  height = 8,
  dpi = 300,
  device = ragg::agg_png,
  bg = "white"
)



```





# ---- Scaled Cycle Day GAMMs Predicting all outcomes on dv_list
- For each variable in @dv_list, 
- Runs GAMM and Hierarchical Variance partitioning hp.gamm()
- Saves it to a .txt file in @current_date's @output_folder in the project's Box Folder for 03_code_dataedits_output. 
- Creates two plots for each variable in @dv_list:
  - Plot 1: Predicted values and confidence intervals for the variable
  - Plot 2: Z-Scored predicted values and confidence intervals for the variable with E2 and P4 curves overlaid
```{r gamm_models_plots}
install.packages("cowplot")  # Install cowplot if needed
library(cowplot)             # Load cowplot for annotations

gamm_models_plots <- function(dv, df, output_folder, plot_path) {
  
  # Step 1: Log-transform variable
  log_dv <- paste0(dv, "_log")
  df[[log_dv]] <- log(df[[dv]] + 1)  

  # Step 2: Fit GAMM model
  gamm_model <- gam(as.formula(paste0(log_dv, " ~ s(id, bs = 're') + s(scaled_cycleday, id, bs = 're') + s(scaled_cycleday, k = 30)")), 
                     data = df, method = "REML")

  # Step 3: Extract model summary & hierarchical partitioning
  model_summary <- summary(gamm_model)
  var_part <- gam.hp::gam.hp(gamm_model)
  
  # Extract p-value and EDF
  scaled_cycleday_row <- which(rownames(model_summary$s.table) == "s(scaled_cycleday)")
  p_value <- model_summary$s.table[scaled_cycleday_row, "p-value"]
  edf <- model_summary$s.table[scaled_cycleday_row, "edf"]

  # Determine significance symbol
  significance <- ifelse(p_value < 0.001, "***", 
                  ifelse(p_value < 0.01, "**", 
                  ifelse(p_value < 0.05, "*", 
                  ifelse(p_value < 0.15, "â€ ", ""))))

  # Save model summary and partitioning results to a file
  summary_output <- capture.output(model_summary)
  partitioning_output <- capture.output(var_part$hierarchical.partitioning)
  combined_output <- c(summary_output, "", "Hierarchical Partitioning:", partitioning_output)
  writeLines(combined_output, file.path(output_folder, paste0("gamm_hp_", dv, ".txt")))

  # Step 4: Create dataset for predictions
  pred_data <- expand.grid(scaled_cycleday = seq(-1, 1, by = 0.1), id = 0)
  pred <- predictions(gamm_model, newdata = pred_data, type = "response", transform = function(x) exp(x) - 1)

  # Add predictions and confidence intervals
  pred_data$estimate <- scale(pred$estimate)
  pred_data$conf.low <- scale(pred$conf.low)
  pred_data$conf.high <- scale(pred$conf.high)

  # Step 5: Create ggplot for predictions
  dv_plot <- ggplot(pred_data, aes(x = scaled_cycleday)) +
    scale_x_continuous(limits = c(-1, 1), breaks = seq(-1, 1, by = 0.50), 
                       labels = c("0%L", "50%L", "Menses Onset", "50%F", "Ovulation")) +
    labs(x = "Cycle Day (Scaled)", y = paste0(dv, " (Z, Pred)")) +
    
    # Background shading for key regions
    geom_rect(aes(xmin = -0.04, xmax = 0.04, ymin = -Inf, ymax = Inf), fill = "grey70", alpha = 0.2) +
    geom_rect(aes(xmin = 0.92, xmax = 1, ymin = -Inf, ymax = Inf), fill = "grey87", alpha = 0.2) +
    
    # Confidence intervals
    geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "black", alpha = 0.1) +
    
    # Main predicted line
    geom_line(aes(y = estimate, color = "dv"), size = 1) +
    
    # Add E2 and P4 curves
    geom_line(data = E2dat, aes(x = scaled_cycleday, y = scale(estimate), color = "E2"), size = 1, linetype = "dashed") +
    geom_line(data = P4dat, aes(x = scaled_cycleday, y = scale(estimate), color = "P4"), size = 1, linetype = "dashed") +
    
    # Customize legend
    scale_color_manual(name = "Key", 
                       values = c("dv" = "black", "E2" = "forestgreen", "P4" = "red"),
                       labels = c("dv" = dv, "E2" = "E2", "P4" = "P4"),
                       breaks = c("dv", "E2", "P4")) +
    
    # Styling with balanced text sizes
    theme_minimal() +
    theme(
      axis.text = element_text(size = 14),    # Slightly reduced from 18
      axis.title = element_text(size = 16),   # Slightly reduced from 20
      legend.text = element_text(size = 14),  # Reduced for better fit
      legend.title = element_text(size = 16)  # Keep title slightly larger
    )

  # Step 6: Annotate with EDF & p-value
  annotation_text <- paste("EDF:", round(edf, 2), significance)
  annotated_plot <- ggdraw(dv_plot) +
    draw_label(annotation_text, x = 0.85, y = 0.2, hjust = 0, size = 10, fontface = "bold")

  # Save the plot as PDF with slightly increased size
  plot_file_path <- file.path(plot_path, paste0(dv, "_scaled_with_horm.pdf"))
  ggsave(filename = plot_file_path, plot = annotated_plot, width = 10, height = 7.5, device = cairo_pdf)
  cat("Plot with Hormone overlays saved to:", plot_file_path, "\n")

  # Step 7: Create another plot without scaling
  pred_data$estimate <- pred$estimate
  pred_data$conf.low <- pred$conf.low
  pred_data$conf.high <- pred$conf.high

  dv_plot_orig <- ggplot(pred_data, aes(x = scaled_cycleday)) +
    scale_x_continuous(limits = c(-1, 1), breaks = seq(-1, 1, by = 0.50), 
                       labels = c("0%L", "50%L", "Menses Onset", "50%F", "Ovulation")) +
    labs(x = "Cycle Day (Scaled)", y = paste0(dv, " (Original Scale)")) +
    
    # Background shading
    geom_rect(aes(xmin = -0.04, xmax = 0.04, ymin = -Inf, ymax = Inf), fill = "grey70", alpha = 0.2) +
    geom_rect(aes(xmin = 0.92, xmax = 1, ymin = -Inf, ymax = Inf), fill = "grey87", alpha = 0.2) +
    
    # Confidence intervals
    geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "black", alpha = 0.1) +
    
    # Main predicted line
    geom_line(aes(y = estimate), color = "black", size = 1) +
    
    theme_minimal() +
    theme(
      axis.text = element_text(size = 14),
      axis.title = element_text(size = 16)
    )

  # Save as PDF
  plot_file_path_orig <- file.path(plot_path, paste0(dv, "_origscale.pdf"))
  ggsave(filename = plot_file_path_orig, plot = dv_plot_orig, width = 10, height = 7.5, device = cairo_pdf)
  cat("Original Scale Plot saved to:", plot_file_path_orig, "\n")
}

# Example Usage
plot_path <- file.path(output_folder, "GAMM_Plots")
for (dv in dv_list) {
  gamm_models_plots(dv, df, output_folder, plot_path)
}



```







## Prepare Hormones

```{r}

#df$logE2 = log(df$E2 + 1)
#df$logP4 = log(df$P4 + 1)

df <- df %>%
  group_by(id) %>%
  mutate(
    E2zd = scale(E2),
    P4zd = scale(P4),
    
    # Calculate 5th and 95th percentiles for E2 and P4 within each id
    E2_lower = quantile(E2zd, 0.05, na.rm = TRUE),
    E2_upper = quantile(E2zd, 0.95, na.rm = TRUE),
    P4_lower = quantile(P4zd, 0.05, na.rm = TRUE),
    P4_upper = quantile(P4zd, 0.95, na.rm = TRUE),
    
    # Manually Winsorize by clamping values to the 5th and 95th percentiles
    E2zd = pmin(pmax(E2zd, E2_lower), E2_upper),
    P4zd = pmin(pmax(P4zd, P4_lower), P4_upper),
    
    # Daily derivatives for E2 and P4
    E2zch = c(NA, diff(E2zd)),  # Winsorized E2 derivative
    P4zch = c(NA, diff(P4zd))   # Winsorized P4 derivative
  ) %>%
  filter(
    !is.na(E2zd) & !is.nan(E2zd) & !is.infinite(E2zd) &
      !is.na(P4zd) & !is.nan(P4zd) & !is.infinite(P4zd) &
      !is.na(E2zch) & !is.nan(E2zch) & !is.infinite(E2zch) &
      !is.na(P4zch) & !is.nan(P4zch) & !is.infinite(P4zch)
  ) %>%
  ungroup()


```

# ---- E2 and P4 GAMMs Predicting all outcomes on dv_list
- Several Variables were created in the dailyprep file: 
- E2.m, E2.sd, E2.d, E2.zd, E2.zd.3roll, E2.zd.5roll (for E2, P4, and LH, as well as everything in @alldailyvars)
-TODO: We need to add lags and consider derivatives and using rolling averages 
-TODO:  HOWEVER, I need the winsorization below. We will need to add the winsorization to the dailyprep file and re-run it.

# Looping through hormone effects - FIXED: E2zd, P4zd, E2zd*P4zd, E2zch, P4zch

```{r}

# Function to Loop Through Outcomes and Generate GAMM Models
create_gamm_models <- function(dv_list, df, save_dir) {
  for (outcome in dv_list) {
    # Prepare variables dynamically
    outcome_log <- paste0(outcome, "log")
    df[[outcome_log]] <- log(df[[outcome]] + 1)

    # Plot histogram for transformed outcome
    hist(df[[outcome]], main = paste("Histogram of", outcome))
    hist(df[[outcome_log]], main = paste("Histogram of Log-transformed", outcome))

    # GAMM model with hormone z-scores and derivatives
    gamm_formula <- as.formula(paste0(
      outcome_log, " ~ s(id, bs = \"re\") + s(E2zd) +",
      " s(P4zd)  + ti(E2zd, P4zd) +",
      " s(E2zch) + s(P4zch)"
    ))

    gamm_model <- gam(
      gamm_formula,
      data = df,
      family = gaussian,
      method = "REML"
    )

    # Summary and plot of the model
    model_summary <- summary(gamm_model)
    print(model_summary)

    # Save the model summary as a text file
    summary_output <- capture.output(model_summary)
    summary_file_path <- file.path(save_dir, paste0(outcome, "_fixed__wzch_85_model_EP_summary.txt"))
    writeLines(summary_output, summary_file_path)
    cat("Model summary saved to:", summary_file_path, "\n")

    # Plot the GAMM effects
    plot.gam(gamm_model, select = 2, main = paste("Estrogen Effect on", outcome))
    plot.gam(gamm_model, select = 3, main = paste("Progesterone Effect on", outcome))
   plot.gam(gamm_model, select = 5, main = paste("Estrogen Change Effect on", outcome))
   plot.gam(gamm_model, select = 6, main = paste("Progesterone Change Effect on", outcome))

    # Simpler model without random effects for visualization
    gamm_simple_formula <- as.formula(paste0(
      outcome_log, " ~ s(E2zd) + s(P4zd) + ti(E2zd, P4zd)"
    ))

    gamm_model_simple <- gam(
      gamm_simple_formula,
      data = df,
      family = gaussian,
      method = "REML"
    )

    # Create the grid for contour plot
    grid <- expand.grid(
      E2zd = seq(
        min(df$E2zd, na.rm = TRUE),
        max(df$E2zd, na.rm = TRUE),
        length.out = 100
      ),
      P4zd = seq(
        min(df$P4zd, na.rm = TRUE),
        max(df$P4zd, na.rm = TRUE),
        length.out = 100
      )
    )

    # Predict values
    grid$predicted <- predict(gamm_model_simple,
                              newdata = grid,
                              type = "response",
                              transform = function(x) exp(x) - 1)

    # Contour plot
    plot_title <- paste("Predicting", outcome, "from E2 and P4 (Person Standardized)")
    contour_plot <- ggplot() +
      geom_tile(data = grid, aes(x = E2zd, y = P4zd, fill = predicted)) +
      geom_contour(data = grid, aes(x = E2zd, y = P4zd, z = predicted), color = "black") +
      geom_point(data = df, aes(x = E2zd, y = P4zd), color = "black", alpha = 0.25) +
      scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = median(grid$predicted)) +
      labs(x = "E2zd", y = "P4zd", fill = "Outcome") +
      theme_minimal() +
      ggtitle(plot_title)

    print(contour_plot)

    # Save the contour plot
    plot_file_path <- file.path(save_dir, paste0(outcome, "_85_wzch_fixed.png"))
    ggsave(filename = plot_file_path, plot = contour_plot, width = 8, height = 6)
    cat("Contour plot saved to:", plot_file_path, "\n")
  }
}

# CALL THE LOOP


create_gamm_models(dv_list, df, output_folder)

```

# Looping through hormone effects - FIXED E2zd*P4zd TENSOR PRODUCT SMOOTH
#TODO - need to figure out how to add the intercept back to the 3dplot because the estimates are wrong due to that

```{r}
library(plotly)
library(htmlwidgets)

# Function to Loop Through Outcomes and Generate GAMM Models
create_gamm_models <- function(dv_list, df, save_dir) {
  for (outcome in dv_list) {
    # Prepare variables dynamically
    outcome_log <- paste0(outcome, "log")
    df[[outcome_log]] <- log(df[[outcome]] + 1)

    # Plot histogram for transformed outcome
    hist(df[[outcome]], main = paste("Histogram of", outcome))
    hist(df[[outcome_log]], main = paste("Histogram of Log-transformed", outcome))

    # GAMM model with hormone z-scores and derivatives
    gamm_formula <- as.formula(paste0(
      outcome_log, " ~ s(id, bs = \"re\") + te(E2zd, P4zd)"
    ))

    gamm_model <- gam(
      gamm_formula,
      data = df,
      family = gaussian,
      method = "REML"
    )

    # Summary and plot of the model
    model_summary <- summary(gamm_model)
    print(model_summary)
    
    # Step 5: Perform hierarchical partitioning
  var_part <- gam.hp::gam.hp(gamm_model)
  print(model_summary)
  print(var_part$hierarchical.partitioning)

  # Step 6: Capture the output for both model summary and hierarchical partitioning
  summary_output <- capture.output(model_summary)
  partitioning_output <- capture.output(var_part$hierarchical.partitioning)

  # Step 7: Combine the model summary and hierarchical partitioning results
  combined_output <- c(summary_output, "", "Hierarchical Partitioning:", partitioning_output)

  # Step 8: Write to file
  summary_file_path <- file.path(output_folder, paste0("gamm_HORMS_hp_", dv, ".txt"))
  writeLines(combined_output, summary_file_path)
    cat("Model summary saved to:", summary_file_path, "\n")

    # Plot the GAMM effects
   # plot.gam(gamm_model, select = 2, main = paste("Estrogen Effect on", outcome))
   # plot.gam(gamm_model, select = 3, main = paste("Progesterone Effect on", outcome))
  # plot.gam(gamm_model, select = 5, main = paste("Estrogen Change Effect on", outcome))
  # plot.gam(gamm_model, select = 6, main = paste("Progesterone Change Effect on", outcome))

    # Simpler model without random effects for visualization
    gamm_simple_formula <- as.formula(paste0(
      outcome_log, " ~ te(E2zd, P4zd)"
    ))

    gamm_model_simple <- gam(
      gamm_simple_formula,
      data = df,
      family = gaussian,
      method = "REML"
    )

    # Create the grid for contour plot
    grid <- expand.grid(
      E2zd = seq(
        min(df$E2zd, na.rm = TRUE),
        max(df$E2zd, na.rm = TRUE),
        length.out = 100
      ),
      P4zd = seq(
        min(df$P4zd, na.rm = TRUE),
        max(df$P4zd, na.rm = TRUE),
        length.out = 100
      )
    )

    # Predict values
    grid$predicted <- predict(gamm_model_simple,
                              newdata = grid,
                              type = "response",
                              transform = function(x) exp(x) - 1)

    # Contour plot
    plot_title <- paste("Predicting", outcome, "from E2 and P4 (Person Standardized)")
    contour_plot <- ggplot() +
      geom_tile(data = grid, aes(x = E2zd, y = P4zd, fill = predicted)) +
      geom_contour(data = grid, aes(x = E2zd, y = P4zd, z = predicted), color = "black") +
      geom_point(data = df, aes(x = E2zd, y = P4zd), color = "black", alpha = 0.25) +
      scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = median(grid$predicted)) +
      labs(x = "E2zd", y = "P4zd", fill = "Outcome") +
      theme_minimal() +
      ggtitle(plot_title)

    print(contour_plot)

    # Save the contour plot
    plot_file_path <- file.path(save_dir, paste0(outcome, "_te_zd_fixed.png"))
    ggsave(filename = plot_file_path, plot = contour_plot, width = 8, height = 6)
    cat("Contour plot saved to:", plot_file_path, "\n")
    
     # Create the 3D surface plot using Plotly
    grid_matrix <- matrix(grid$predicted, nrow = 100, byrow = TRUE)
    surface_plot <- plot_ly(
      x = seq(min(df$E2zd, na.rm = TRUE), max(df$E2zd, na.rm = TRUE), length.out = 100),
      y = seq(min(df$P4zd, na.rm = TRUE), max(df$P4zd, na.rm = TRUE), length.out = 100),
      z = ~grid_matrix
    ) %>%
      add_surface() %>%
      layout(
        title = paste("3D Surface Plot of E2zd and P4zd Interaction for", outcome),
        scene = list(
          xaxis = list(title = "E2zd"),
          yaxis = list(title = "P4zd"),
          zaxis = list(title = "Predicted Outcome")
        )
      )

    # Save the interactive surface plot as an HTML file
    surface_plot_file_path <- file.path(save_dir, paste0(outcome, "_zd_3D_surface.html"))
    saveWidget(surface_plot, surface_plot_file_path)
    cat("3D surface plot saved to:", surface_plot_file_path, "\n")
  }
}


# CALL THE LOOP

create_gamm_models(dv_list, df=df, save_dir=output_folder)

```






Applying examples from "mixed_effects_gamms" repo


```{r S_cycle, fig.width=8, fig.height=5}
# Set id as factor
df$id <- as.factor(df$id)

S_model <- gam(score_pinball ~ s(scaled_cycleday, bs = "cc") +
                       s(id, bs = "re") +
                       ti(scaled_cycleday, id, bs = c("cc", "re")),
                     data = df, knots=list(scaled_cycleday=c(-1, 1)),
                     family = gaussian(), 
                     method = "REML")

summary(S_model)
gam.check(S_model)



```

# IV: Examples

```{r part_4_preamble, include=FALSE}
#### Code for IV: Examples ####
```
We now demonstrate two worked examples on one data set to highlight how to use HGAMs in practice, and to illustrate how to fit, test, and
visualize each model. We will demonstrate how to use these models to fit community
data, to show when using a global trend may or may not be justified, and to
illustrate how to use these models to fit seasonal time series.

For these examples, data are from a long-term study in seasonal dynamics of zooplankton, collected by the Richard Lathrop. The data were collected from a chain of lakes in Wisconsin (Mendota, Monona, Kegnonsa, and Waubesa) approximately bi-weekly from 1976 to 1994. They consist of samples of the zooplankton communities, taken from the deepest point of each lake via vertical tow. The data are provided by the Wisconsin Department of Natural Resources and their collection and processing are fully described in @lathrop_madison_2000. 


Zooplankton in temperate lakes often undergo seasonal cycles, where the abundance of each species fluctuates up and down across the course of the year, with each species typically showing a distinct pattern of seasonal cycles. The inferential aims of these examples are to *(i)* estimate variability in seasonality among species in the community in a single lake (Mendota), and *(ii)* estimate among-lake variability for the most abundant taxon in the sample (*Daphnia mendotae*) across the four lakes. To enable evaluation of out-of-sample performance, we split the data into testing and training sets. As there are multiple years of data, we used data from the even years to fit (train) models, and the odd years to test the fit.

Each record consists of counts of a given zooplankton taxon taken from a subsample from a single vertical net tow, which was then scaled to account for the relative volume of subsample versus the whole net sample and the area of the net tow, giving  population density per $m^2$.  Values are rounded to the nearest 1000. Observed densities span four orders of magnitude.  We modelled density using a Gamma distribution with a log-link. For any net tow sample where a given taxon was not observed, we set that taxon's density to 1000 (the minimum possible sample size)[^censored]. 
To evaluate how well each model fits new data (not used to fit the model), we calculated the total deviance of the out-of-sample data that we had previously held out. The deviance is equal to two times the sum of the difference between the log-likelihood of the out-of-sample data (as predicted by each model) and a saturated model, that has one predictor for each data point, all multiplied by the scale parameter for the family of interest. It can be interpreted similarly to the residual sum of squares for a simple linear regression [@wood_generalized_2017, page 109]. 


[^censored]: A more appropriate model for this data would be to assume that density is *left censored*, where 1000 is treated as a threshold which the data may lie below, but it is not possible to measure lower than this. However, **mgcv** does not currently have a left-censored family. The **brms** package, for Bayesian model fitting, can fit a left-censored Gamma distribution, so it would be possible to fit this model using that software. We discuss using HGAMs in **brms** in section V. 



```{r view_zoo, include = FALSE, message=FALSE,  cache=TRUE}

zooplankton <- read.csv("data/zooplankton_example.csv",stringsAsFactors = TRUE)%>%
  mutate(year_f = factor(year))

#This is what the data looks like:
str(zooplankton)
levels(zooplankton$taxon)
levels(zooplankton$lake)

# We'll now break it into testing and training data. The training data will be
# used to fit the model, and the testing data will be used to evaluate model
# fit.

#the first training and testing data set will be used to compare dynamics of
#plankton communities in Lake Mendota
zoo_train <- subset(zooplankton, year%%2==0 & lake=="Mendota")
zoo_test  <- subset(zooplankton, year%%2==1 & lake=="Mendota") 

#The second training and testing set will compare Daphnia mendotae dynamics
#among four lakes
daphnia_train <- subset(zooplankton, year%%2==0 & taxon=="D. mendotae")
daphnia_test  <- subset(zooplankton, year%%2==1 & taxon=="D. mendotae")

#This function calculates the deviance of out-of-sample data,
#conditional on their mean predicted value from the model
get_deviance <- function(model, y_pred, y_obs, weights = NULL){
  stopifnot(length(y_obs)==length(y_pred))
  #We don't use the weights term in this paper, but it can be useful if
  #how well the model matters more for some sample points than others
  if(is.null(weights)) weights = rep(1, times= length(y_obs))
  #this uses the deviance residual function from the model family to
  #calculate deviances for individual points
  dev_residuals = model$family$dev.resids(y_obs, y_pred, weights)
  return(sum(dev_residuals))
}
```

First, we demonstrate how to model community-level variability in seasonality, by regressing scaled density on day of year with species-specific curves. As we are not interested in average seasonal dynamics, we focus on models *S* and *I* (if we wanted to estimate the seasonal dynamics for rarer species, adding a global smooth term might be useful, so we could borrow information from the more common species). As the data are seasonal,  we use cyclic smoothers as the basis for seasonal dynamics.  Therefore we need to specify start and end points for our cycles using the `knots` argument to `gam()`, as well as specify this smoother type as a factor-smooth interaction term using the `xt` argument (the `xt` argument is how any extra information that a smoother might need is supplied; see `?mgcv::s` for more information). Note that we also include a random effect smoother for both `taxon` and `taxon:year_f`, where `year_f` is `year` transformed into a factor variable. This deals with the fact that average zooplankton densities can show large year-to-year variation. The argument `drop.unused.levels=FALSE` is also included so the `gam` function does not drop the year factor levels corresponding to those in the held-out test data set. 


### Model *S*:

```{r zoo_comm_modS, echo=TRUE, message=FALSE, cache=TRUE, fig.width=8, fig.height=5}

zoo_comm_modS <- gam(density_adj ~ s(taxon, year_f, bs="re") + #random effect for each taxon
                       s(day, taxon, bs="fs", k=10, xt=list(bs="cc")), # cyclic smoother for day that starts and ends at the same point and has 10 basis functions and a factor-smooth interaction term
                     data=zoo_train, knots=list(day=c(0, 365)), 
                     family=Gamma(link="log"), method="REML",
                     drop.unused.levels=FALSE)
```


### Transferring/copying to our cycle effect example

```{r S_cycle, fig.width=8, fig.height=5}
# Set id as factor
df$id <- as.factor(df$id)

S_model <- gam(score_pinball ~ s(scaled_cycleday, bs = "cc") +
                       s(id, bs = "re") +
                       ti(scaled_cycleday, id, bs = c("cc", "re")),
                     data = df, knots=list(scaled_cycleday=c(-1, 1)
                     family = gaussian(), 
                     method = "REML")

summary(S_model)
```


### Model *I*:

```{r zoo_comm_modI, echo=TRUE, message=FALSE,  cache=TRUE, fig.width=8, fig.height=5}

# Note that s(taxon, bs="re") has to be explicitly included here, as the 
# day by taxon smoother does not include an intercept
zoo_comm_modI <- gam(density_adj ~ s(day, by=taxon, k=10, bs="cc") + 
                       s(taxon, bs="re") + s(taxon, year_f, bs="re"),
                     data=zoo_train, knots=list(day=c(0, 365)),
                     family=Gamma(link="log"), method="REML",
                     drop.unused.levels=FALSE)
```

At this stage of the analysis (prior to model comparisons), it is useful to determine if any of the fitted models adequately describe patterns in the data (i.e. goodness of fit testing). 
**mgcv**'s `gam.check()` facilitates this model-checking. 
This function creates a set of standard diagnostic plots: a QQ plot of the deviance residuals (see @wood_generalized_2017) compared to their theoretical expectation for the chosen family, a plot of response versus fitted values, a histogram of residuals, and a plot of residuals versus fitted values. It also conducts a test for each smooth term to determine if the number of degrees of freedom (`k`) for each smooth is adequate (see `?mgcv::gam.check` for details on how that test works). The code for checking model *S* and *I* for the community zooplankton model is: 

```{r zoo_comm_modI_check_gam,eval = FALSE, echo=TRUE,  message=FALSE}

gam.check(zoo_comm_modS)
gam.check(zoo_comm_modI)
```

We have plotted QQ plots and fitted-versus residual plots for model *I* (fitted versus response plots are generally less useful for non-normally distributed data as it can be difficult to visually assess if the observed data shows more heteroskedasticity than expected). The results for model *S* are virtually indistinguishable to the naked eye. We have also used alternate QQ-plotting code from  the **gratia** package [@simpson_gratia_2018], using the `qq_plot()` function, as this function creates a **ggplot2** object that are easier to customize than the **base** plots from `gam.check()`. The code for generating these plots is in the supplemental material.
These plots (Fig. \ref{fig:Fig14}) indicate that the Gamma distribution seems to fit the observed data well except at low values, where the deviance residuals are larger than predicted by the theoretical quantiles (Fig. \ref{fig:Fig14}A). 
There also does not seem to be a pattern in the residual versus fitted values (Fig. \ref{fig:Fig14}B), except for a line of residuals at the lowest values, which correspond to all of those observations where a given taxon was absent from the sample.

```{r Fig14, echo=FALSE, message=FALSE, warning=TRUE, cache=TRUE,results="markup", fig.width=8, fig.height=3, fig.cap = "\\label{fig:Fig14} Diagnostic plots for model *I* fitted to zooplankton community data in Lake Mendota. A) QQ-plot of residuals (black). Red line indicates the 1-1 line and grey bands correspond to the expected 95% CI for the QQ plot, assuming the distribution is correct.  B) Deviance residuals versus fitted values (on the link scale)."}

#Checking residuals and qqplots for GAM fits

#QQ-plot, using gratia's qq_plot function, with simulated confidence intervals.
#We are removing the title and subtitle to simplify the figure
plt1 <- qq_plot(zoo_comm_modI, method = "simulate") +
  labs(title =NULL, subtitle =NULL)
df <- data.frame(log_fitted = log(fitted(zoo_comm_modI)),
                 residuals  = resid(zoo_comm_modI, type = "deviance"))

#fitted versus deviance plot
plt2 <- ggplot(df, aes(x = log_fitted, y = residuals)) +
    geom_point() +
    labs(x = "Linear predictor", y = "Deviance residual")
plot_grid(plt1, plt2, ncol = 2, align = "hv", axis = "lrtb",labels=c("A","B"))
```


The `k.check()` test (Table \ref{tab:zoo_comm_check_k_kable}) shows that the default maximum degrees of freedom for the smoothers used in model *I* are sufficient for all species, as the effective degrees of freedom (EDF) for all estimated smoothers are well below their maximum possible value (k'), and the  p-value for the observed k-index (which measures pattern in the residuals) is not significant. 

```{r zoo_comm_check_k, echo=FALSE,eval = FALSE, message=FALSE, warning=TRUE, cache=TRUE,results="markup"}

#individual components of gam.check: the results for k.check
round(k.check(zoo_comm_modI),2)
```


```{r zoo_comm_check_k_kable, echo=FALSE, message=FALSE, cache=TRUE, purl=FALSE}
k.check_table <-round(k.check(zoo_comm_modI),2) %>%
  as.data.frame()%>%
  rownames_to_column()%>%
  rename(`Model term` = rowname,
         EDF = edf)

kable(k.check_table, 
      format = table_out_format,
      caption='Results from running \\texttt{k.check()} on \\texttt{zoo\\_comm\\_modI}. Each row corresponds to a single model term. The notation for term names uses \\textbf{mgcv} syntax. For instance, "s(day):taxonC. sphaericus" refers to the the smoother for day for the taxon \\textit{C. sphaericus}.', 
      booktabs = TRUE,
      escape = TRUE)%>%
  kable_styling(full_width = FALSE) %>%
  row_spec(2:3,italic = FALSE) %>%
  row_spec(2:3, italic = FALSE)
  
```


In this table, each row corresponds to a single smooth term, k' corresponds to the number of basis functions used for that smoother in the fitted model (smaller than the specified `k` in the model itself, as some basis functions are automatically dropped to ensure the model is identifiable). The column EDF is the estimated Effective Degrees of Freedom for that smoother, the k-index is a measure of the remaining pattern in the residuals, and the p-value is calculated based on the distribution of the k-index after randomizing the order of the residuals. Note that there is no p-value for the random effects smoothers `s(taxon)` and `s(taxon,year_f)` as the p-value is calculated from simulation-based tests for autocorrelation of the residuals. As `taxon` and `year_f` are treated as simple random effects with no natural ordering, there is no meaningful way of checking for autocorrelation. 


Differences between models *S* (shared smoothness between taxa) and *I* (different smoothness for each taxa) seem to be driven by the low seasonality of *L. siciloides* relative to the other species, and how this is captured by the more flexible model *I* (Fig. \ref{fig:Fig15}). Still, both models show very similar fits to the training data. This implies that the added complexity of different penalties for each species (model *I*) is unnecessary here, which is consistent with the fact that model *S* has a lower AIC (`r round(AIC(zoo_comm_modS), 0)`) than model *I* (`r round(AIC(zoo_comm_modI), 0)`), and that model *S* is somewhat better at predicting out-of-sample fits for all taxa than model *I* (Table \ref{tab:zoo_comm_outofsample_kable}). Both models show significant predictive improvement compared to the intercept-only model for all species except *K. cochlearis* (Table \ref{tab:zoo_comm_outofsample_kable}). This may be driven by changing timing of the spring bloom for this species between training  and out-of-sample years (Fig. \ref{fig:Fig15}). 


```{r Fig15, echo=FALSE, message=FALSE, warning=TRUE, cache=TRUE,results="markup", fig.width=6, fig.height=6, fig.cap = "\\label{fig:Fig15}Species-specific seasonal dynamics for the eight zooplankon species tracked in Lake Mendota. Black points indicate individual plankton observations in the training data, and grey points are observations in held-out years used for model validation. Lines indicate predicted average values for model *S* (green) and model *I* (red). Ribbons indicate $\\pm$ 2 standard errors around the mean."}

#Create synthetic data to use to compare predictions
zoo_plot_data <- expand.grid(day = 1:365, 
                             taxon = factor(levels(zoo_train$taxon)), 
                             year_f = 1980)

#extract predicted values and standard errors for both models. 
#the exclude = "s(taxon,year_f)" term indicates that predictions should be made
#excluding the effect of the taxon by year random effect (effectively setting
#making predictions averaging over year-taxon effects).
zoo_modS_fit <- predict(zoo_comm_modS, 
                        zoo_plot_data, 
                        se.fit = TRUE, 
                        exclude = "s(taxon,year_f)")
zoo_modI_fit <- predict(zoo_comm_modI, 
                        zoo_plot_data, 
                        se.fit = TRUE, 
                        exclude = "s(taxon,year_f)")

zoo_plot_data$modS_fit <- as.numeric(zoo_modS_fit$fit)
zoo_plot_data$modI_fit <- as.numeric(zoo_modI_fit$fit)

zoo_plot_data <- gather(zoo_plot_data, model, fit, modS_fit, modI_fit)
zoo_plot_data <- mutate(zoo_plot_data, se= c(as.numeric(zoo_modS_fit$se.fit),
                                             as.numeric(zoo_modI_fit$se.fit)),
                        upper = exp(fit + (2 * se)),
                        lower = exp(fit - (2 * se)),
                        fit   = exp(fit))

#Plot the model output, with means plus standard deviations for each model.
zoo_plot_model_labels = paste("Model", c("S","I"))
zoo_plot_model_labels = factor(zoo_plot_model_labels, 
                               levels = zoo_plot_model_labels)

zoo_plot <- ggplot(zoo_plot_data) +
  facet_wrap(~taxon, nrow = 4,scales = "free_y")+
  geom_ribbon(aes(x=day,
                  ymin = lower,
                  ymax = upper,
                  fill = model),
              alpha=0.2)+
  geom_point(data= zoo_train, aes(x = day, y = density_adj),size=0.06)+
  geom_point(data= zoo_test, aes(x = day, y = density_adj),
             size=0.06,col="grey")+
  geom_line(aes(x = day, y = fit, color = model))+
  labs(y = expression(atop(Population~density,("10 000"~individuals~m^{-2}))), 
       x = "Day of Year") +
  scale_y_log10(breaks = c(0.1,1,10,100, 1000), 
                labels = c("0.1","1","10","100","1000"))+
  scale_fill_brewer(name = "", palette = "Dark2",
                      labels = zoo_plot_model_labels) +
  scale_colour_brewer(name = "",
                        palette = "Dark2", labels = zoo_plot_model_labels)+
  theme(legend.position = "top")

zoo_plot
```



```{r zoo_comm_outofsample, echo=FALSE, message=FALSE, dependson='view_zoo', cache=TRUE}

#Getting the out-of-sample predictions for both models:

# we need to compare how well this model fits with a null model. here we'll use an
# intercept-only model
zoo_comm_mod0 <- gam(density_adj ~ s(taxon,bs="re"),
                     data=zoo_train,
                     knots = list(day =c(0, 365)),
                     family = Gamma(link ="log"), 
                     method = "REML",
                     drop.unused.levels = FALSE)

#Correlations between fitted and observed values for all species:
#\n is in variable titles to add a line break in the printed table.
zoo_test_summary = zoo_test %>%
  mutate(
    mod0 = predict(zoo_comm_mod0, ., type="response"),
    modS = predict(zoo_comm_modS, ., type="response"),
    modI = predict(zoo_comm_modI, ., type="response"))%>%
  group_by(taxon)%>%
  summarise(
    `Intercept only` = format(get_deviance(zoo_comm_mod0, mod0, density_adj), 
                              scientific = FALSE, 
                              digits=3),
    `Model S` = format(get_deviance(zoo_comm_modS, modS, density_adj), 
                       scientific = FALSE, 
                       digits=3),
    `Model I` = format(get_deviance(zoo_comm_modI, modI, density_adj), 
                       scientific = FALSE, 
                       digits=3))

```

```{r zoo_comm_outofsample_kable, echo=FALSE, message=FALSE, dependson=-1, cache=TRUE,purl=FALSE}

zoo_test_summary = zoo_test_summary%>%
  #need to specify this to ensure that species names are italized in the table
  mutate(taxon = cell_spec(taxon, 
                           italic = c(TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE)))%>%
  #Capitilizing taxon in the table names
  rename(Taxon = taxon)

kable(zoo_test_summary, 
      format = table_out_format, 
      caption="Out-of-sample predictive ability for model \\textit{S} and \\textit{I} applied to the zooplankton community dataset. Deviance values represent the total deviance of model predictions from observations for out-of-sample data. 'Intercept only' results are for a null model with only taxon-level random effect intercepts included.", 
      booktabs = TRUE,
      escape = FALSE)%>%
  add_header_above(c(" " = 1, "Total deviance of out-of-sample data" = 3),
                   escape = FALSE)%>%
  kable_styling(full_width = FALSE) %>%
  row_spec(2:3,italic = FALSE) %>%
  row_spec(2:3, italic = FALSE)
  
```

Next, we look at how to fit inter-lake variability in dynamics for just *Daphnia mendotae*.
Here, we will compare models *G*, *GS*, and *GI* to determine if a single global function is appropriate for all four lakes, or if we can more effectively model variation between lakes with a shared smoother and lake-specific smoothers.

### Model *G*:

```{r zoo_daph_modG, echo=TRUE, message=FALSE, cache=TRUE}

zoo_daph_modG <- gam(density_adj ~ s(day, bs="cc", k=10) + s(lake, bs="re") +
                       s(lake, year_f, bs="re"),
                     data=daphnia_train, knots=list(day=c(0, 365)),
                     family=Gamma(link="log"), method="REML",
                     drop.unused.levels=FALSE)
```

### Model *GS*:
```{r zoo_daph_modGS, echo=TRUE, message=FALSE,  cache=TRUE}

zoo_daph_modGS <- gam(density_adj ~ s(day, bs="cc", k=10) +
                        s(day, lake, k=10, bs="fs", xt=list(bs="cc")) +
                        s(lake, year_f, bs="re"),
                      data=daphnia_train, knots=list(day=c(0, 365)),
                      family=Gamma(link="log"), method="REML",
                      drop.unused.levels=FALSE)
```

### Model *GI*:

```{r zoo_daph_modGI, echo=TRUE, message=FALSE,  cache=TRUE}

zoo_daph_modGI <- gam(density_adj~s(day, bs="cc", k=10) + s(lake, bs="re") +
                        s(day, by=lake, k=10, bs="cc") +
                        s(lake, year_f, bs="re"),
                      data=daphnia_train, knots=list(day=c(0, 365)),
                      family=Gamma(link ="log"), method="REML",
                      drop.unused.levels=FALSE)
```

Diagnostic plots from `gam.check()` indicate that there are no substantial patterns comparing residuals to fitted values (not shown), and QQ-plots are similar to those from the zooplankton community models; the residuals for all three models closely correspond to the expected (Gamma) distribution, except at small values, where the observed residuals are generally larger than expected (Fig. \ref{fig:Fig16}). 
As with the community data, this is likely an artifact of the assumption we made of assigning zero observations a value of 1000 (the lowest possible value), imposing an artificial lower bound on the observed counts. There was also some evidence that the largest observed values were smaller than expected given the theoretical distribution , but these fell within the 95% CI for expected deviations from the 1-1 line (Fig. \ref{fig:Fig16}). 

```{r Fig16, echo=FALSE, message=FALSE, warning=TRUE, cache=TRUE,results="markup", fig.width=9, fig.height=3, fig.cap = "\\label{fig:Fig16} QQ-plots for model *G* (A) , *GS* (B), and *GI* (C) fitted to Daphnia data across the four lakes. Red line indicates the 1-1 line, black points are observed model residuals, and grey bands correspond to the expected 95% CI for the QQ plot, assuming the distribution is correct."}

#Checking residuals and qqplots for GAM fits

#qqplot, using gratia's qq_plot function, with simulated confidence intervals
pltG <- qq_plot(zoo_daph_modG, method = "simulate")+
  labs(subtitle = NULL, title=NULL)
pltGS <- qq_plot(zoo_daph_modGS, method = "simulate")+
  labs(subtitle = NULL, title=NULL, y=NULL)
pltGI <- qq_plot(zoo_daph_modGI, method = "simulate")+
  labs(subtitle = NULL, title=NULL, y=NULL)

plot_grid(pltG, pltGS,pltGI, 
          ncol = 3, 
          align = "hv", 
          axis = "lrtb",labels=c("A","B","C"))
```


AIC values indicate that both model *GS* (`r round(AIC(zoo_daph_modGS), 2)`) and *GI*
(`r round(AIC(zoo_daph_modGI), 2)`) are better fits than model *G* (`r round(AIC(zoo_daph_modG), 2)`),
with model *GI* fitting somewhat better than model *GS* [^AIC_note].  There does not seem to be a large amount of inter-lake variability (the effective degrees of freedom per lake are low in models *GS* & *GI*).  Plots for all three models
(Fig. \ref{fig:Fig17}) show that Mendota, Monona, and Kegonsa lakes are very close to the average and to one another for both models, but Waubesa shows evidence of a more pronounced spring bloom and lower winter abundances. 

[^AIC_note]: When comparing models via AIC, we use the standard rule of thumb from @burnham_model_1998, where models that differ by 2 units or less from the lowest AIC model have substantial support, and those differing by more than 4 units have less support. 


```{r Fig17, echo=FALSE, message=FALSE, warning=TRUE, cache=TRUE, fig.width=6, fig.height=4, fig.cap="\\label{fig:Fig17}Raw data (points) and fitted models (lines) for \\textit{D. mendota} data.  Black points indicate individual plankton observations in the training data, and grey points are observations in held-out years used for model validation. Green line: model *G* (no inter-lake variation in dynamics); orange line: model *GS* (interlake variation with similar smoothness); purple line: model *GI* (varying smoothness among lakes). Shaded bands are drawn at $\\pm$ 2 standard errors around each model."}

#Create synthetic data to use to compare predictions
daph_plot_data <- expand.grid(day = 1:365, 
                              lake = factor(levels(zoo_train$lake)),
                              year_f = 1980)

#extract predicted values and standard errors for both models. the 
#exclude ="s(taxon,year_f)" term indicates that predictions should be made 
#excluding the effect of the taxon-by-year random effect (effectively making
#predictions averaging over year-taxon effects).
daph_modG_fit <- predict(zoo_daph_modG, 
                         newdata = daph_plot_data, 
                         se.fit = TRUE, 
                         exclude = "s(lake,year_f)")
daph_modGS_fit <- predict(zoo_daph_modGS, 
                         newdata = daph_plot_data, 
                         se.fit = TRUE, 
                         exclude = "s(lake,year_f)")
daph_modGI_fit <- predict(zoo_daph_modGI, 
                         newdata = daph_plot_data, 
                         se.fit = TRUE, 
                         exclude = "s(lake,year_f)")

daph_plot_data$modG_fit <- as.numeric(daph_modG_fit$fit)
daph_plot_data$modGS_fit <- as.numeric(daph_modGS_fit$fit)
daph_plot_data$modGI_fit <- as.numeric(daph_modGI_fit$fit)

daph_plot_data <- gather(daph_plot_data, 
                         key = model, 
                         value = fit, 
                         modG_fit, 
                         modGS_fit, 
                         modGI_fit)

daph_plot_data <- mutate(daph_plot_data, 
                         se = c(as.numeric(daph_modG_fit$se.fit),
                                as.numeric(daph_modGS_fit$se.fit),
                                as.numeric(daph_modGI_fit$se.fit)),
                         upper = exp(fit + (2 * se)),
                         lower = exp(fit - (2 * se)),
                         fit   = exp(fit))

daph_plot_model_labels = paste("Model", c("G","GS","GI"))
daph_plot_model_labels = factor(daph_plot_model_labels, 
                                levels= daph_plot_model_labels)

daph_plot <- ggplot(daph_plot_data, aes(x=day))+
  facet_wrap(~lake, nrow = 2)+
  geom_ribbon(aes(x = day, ymin = lower, ymax = upper, fill = model), 
                alpha = 0.2) +
  geom_point(data= daphnia_train, 
             aes(x = day, 
                 y = density_adj),
             size=0.06)+
  geom_point(data= daphnia_test, 
             aes(x = day, 
                 y = density_adj),
             size=0.06,
             col="grey")+
  geom_line(aes(x = day, y = fit, colour = model)) +

  labs(y = expression(atop(Population~density,
                           ("10 000"~individuals~m^{-2}))), 
       x = "Day of Year") +
  scale_x_continuous(expand = c(0,0))+
  scale_y_log10()+
    scale_fill_brewer(name = "", 
                      palette = "Dark2",
                      labels = daph_plot_model_labels) +
    scale_colour_brewer(name = "",
                        palette = "Dark2", 
                        labels = daph_plot_model_labels)+
  theme(legend.position = "top")


daph_plot
```

Model *GI* is able to predict as well or better than model *G* or *GS* for all lakes (Table \ref{tab:zoo_daph_outofsample_kable}), indicating that allowing for inter-lake variation in seasonal dynamics improved model prediction. All three models predicted dynamics in Lake Mendota and Lake Menona significantly better than the intercept-only model (Table \ref{tab:zoo_daph_outofsample_kable}). None of the models did well in terms of predicting Lake Waubesa dynamics out-of-sample compared to a simple model with only a lake-specific intercept and no intra-annual variability, but this was due to the influence of a single large outlier in the out-of-sample data that occurred after the spring bloom, at day 243 (Fig. \ref{fig:Fig17}; note that the y-axis is log-scaled). 
However, baring a more detailed investigation into the cause of this large value, we cannot arbitrarily exclude this outlier from the goodness-of-fit analysis; 
it may be due either to measurement error or a true high late-season *Daphnia* density that our model was not able to predict.

```{r zoo_daph_outofsample, echo=FALSE, message=FALSE,  cache=TRUE}

# we need to compare how well this model fits with a null model. here we'll use
# an intercept-only model
zoo_daph_mod0 <- gam(density_adj~s(lake, bs="re"),
                     data=daphnia_train,
                     knots=list(day =c(0, 365)),
                     family=Gamma(link ="log"),
                     method="REML",
                     drop.unused.levels = FALSE)



# We'll look at the correlation between fitted and observed values for all species:

daph_test_summary <- daphnia_test %>%
  mutate(
    #get out-of-sample predicted fits
    mod0 = as.numeric(predict(zoo_daph_mod0,.,type="response")),
    modG = as.numeric(predict(zoo_daph_modG,.,type="response")),
    modGS = as.numeric(predict(zoo_daph_modGS,.,type="response")),
    modGI = as.numeric(predict(zoo_daph_modGI,.,type="response")))%>%
  group_by(lake)%>%
  summarise(`Intercept only` = format(get_deviance(zoo_daph_mod0, 
                                                   mod0, 
                                                   density_adj), 
                                      scientific = FALSE, 
                                      digits=2),
            `Model G` = format(get_deviance(zoo_daph_modG, 
                                            modG, 
                                            density_adj), 
                               scientific = FALSE, 
                               digits=2),
            `Model GS` = format(get_deviance(zoo_daph_modGS, 
                                             modGS, 
                                             density_adj), 
                               scientific = FALSE, 
                               digits=2),
            `Model GI` = format(get_deviance(zoo_daph_modGI, 
                                             modGI, 
                                             density_adj), 
                               scientific = FALSE, 
                               digits=2))%>%
  rename(Lake = lake)
```

```{r zoo_daph_outofsample_kable, echo=FALSE, message=FALSE, dependson=-1,  cache=TRUE, purl = FALSE}
kable(daph_test_summary,
      format = table_out_format, 
      caption="Out-of-sample predictive ability for model \\textit{G}, \\textit{GS}, and \\textit{GI} applied to the \\textit{D. mendotae} dataset. Deviance values represent the total deviance of model predictions from observations for out-of-sample data. 'Intercept only' results are for a null model with only lake-level random effect intercepts included.", 
      booktabs = TRUE)%>%
  add_header_above(c(" " = 1, "Total deviance of out-of-sample data" = 4),
                   escape = FALSE)%>%
  kable_styling(full_width = FALSE)
```
